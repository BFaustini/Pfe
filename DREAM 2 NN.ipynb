{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparation des donnés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faire un tableau de best prediction de tous les descritpteurs par rapport a celle bien predite par le random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numerical libraries.  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, MultiLabelBinarizer\n",
    "\n",
    "#Librairies\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "\n",
    "#Raccourcis utilisé\n",
    "from tqdm import tqdm_notebook\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CID</th>\n",
       "      <th>complexity from pubmed</th>\n",
       "      <th>MW</th>\n",
       "      <th>AMW</th>\n",
       "      <th>Sv</th>\n",
       "      <th>Se</th>\n",
       "      <th>Sp</th>\n",
       "      <th>Si</th>\n",
       "      <th>Mv</th>\n",
       "      <th>...</th>\n",
       "      <th>ACID</th>\n",
       "      <th>WARM</th>\n",
       "      <th>MUSKY</th>\n",
       "      <th>SWEATY</th>\n",
       "      <th>AMMONIA/URINOUS</th>\n",
       "      <th>DECAYED</th>\n",
       "      <th>WOOD</th>\n",
       "      <th>GRASS</th>\n",
       "      <th>FLOWER</th>\n",
       "      <th>CHEMICAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>4.532</td>\n",
       "      <td>4.961</td>\n",
       "      <td>2.011</td>\n",
       "      <td>2.155</td>\n",
       "      <td>2.482</td>\n",
       "      <td>2.168</td>\n",
       "      <td>2.554</td>\n",
       "      <td>0.873</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>4.532</td>\n",
       "      <td>4.961</td>\n",
       "      <td>2.011</td>\n",
       "      <td>2.155</td>\n",
       "      <td>2.482</td>\n",
       "      <td>2.168</td>\n",
       "      <td>2.554</td>\n",
       "      <td>0.873</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>176</td>\n",
       "      <td>3.141</td>\n",
       "      <td>3.916</td>\n",
       "      <td>1.958</td>\n",
       "      <td>1.648</td>\n",
       "      <td>2.034</td>\n",
       "      <td>1.642</td>\n",
       "      <td>2.099</td>\n",
       "      <td>0.824</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>176</td>\n",
       "      <td>3.141</td>\n",
       "      <td>3.916</td>\n",
       "      <td>1.958</td>\n",
       "      <td>1.648</td>\n",
       "      <td>2.034</td>\n",
       "      <td>1.642</td>\n",
       "      <td>2.099</td>\n",
       "      <td>0.824</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>177</td>\n",
       "      <td>2.175</td>\n",
       "      <td>3.531</td>\n",
       "      <td>1.846</td>\n",
       "      <td>1.556</td>\n",
       "      <td>1.921</td>\n",
       "      <td>1.584</td>\n",
       "      <td>2.003</td>\n",
       "      <td>0.813</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>801</td>\n",
       "      <td>6429333</td>\n",
       "      <td>5.336</td>\n",
       "      <td>5.268</td>\n",
       "      <td>1.909</td>\n",
       "      <td>2.372</td>\n",
       "      <td>2.747</td>\n",
       "      <td>2.425</td>\n",
       "      <td>2.855</td>\n",
       "      <td>0.859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>802</td>\n",
       "      <td>6999977</td>\n",
       "      <td>4.657</td>\n",
       "      <td>5.268</td>\n",
       "      <td>1.826</td>\n",
       "      <td>2.341</td>\n",
       "      <td>2.891</td>\n",
       "      <td>2.392</td>\n",
       "      <td>3.019</td>\n",
       "      <td>0.811</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>803</td>\n",
       "      <td>6999977</td>\n",
       "      <td>4.657</td>\n",
       "      <td>5.268</td>\n",
       "      <td>1.826</td>\n",
       "      <td>2.341</td>\n",
       "      <td>2.891</td>\n",
       "      <td>2.392</td>\n",
       "      <td>3.019</td>\n",
       "      <td>0.811</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>804</td>\n",
       "      <td>16220109</td>\n",
       "      <td>5.867</td>\n",
       "      <td>5.670</td>\n",
       "      <td>1.804</td>\n",
       "      <td>2.579</td>\n",
       "      <td>3.128</td>\n",
       "      <td>2.657</td>\n",
       "      <td>3.275</td>\n",
       "      <td>0.821</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>805</td>\n",
       "      <td>16220109</td>\n",
       "      <td>5.867</td>\n",
       "      <td>5.670</td>\n",
       "      <td>1.804</td>\n",
       "      <td>2.579</td>\n",
       "      <td>3.128</td>\n",
       "      <td>2.657</td>\n",
       "      <td>3.275</td>\n",
       "      <td>0.821</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2418 rows × 3104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0       CID  complexity from pubmed     MW    AMW     Sv     Se  \\\n",
       "0             0       126                   4.532  4.961  2.011  2.155  2.482   \n",
       "1             1       126                   4.532  4.961  2.011  2.155  2.482   \n",
       "2             2       176                   3.141  3.916  1.958  1.648  2.034   \n",
       "3             3       176                   3.141  3.916  1.958  1.648  2.034   \n",
       "4             4       177                   2.175  3.531  1.846  1.556  1.921   \n",
       "..          ...       ...                     ...    ...    ...    ...    ...   \n",
       "801         801   6429333                   5.336  5.268  1.909  2.372  2.747   \n",
       "802         802   6999977                   4.657  5.268  1.826  2.341  2.891   \n",
       "803         803   6999977                   4.657  5.268  1.826  2.341  2.891   \n",
       "804         804  16220109                   5.867  5.670  1.804  2.579  3.128   \n",
       "805         805  16220109                   5.867  5.670  1.804  2.579  3.128   \n",
       "\n",
       "        Sp     Si     Mv  ...  ACID  WARM  MUSKY  SWEATY  AMMONIA/URINOUS  \\\n",
       "0    2.168  2.554  0.873  ...   1.0   1.0    1.0     1.0              1.0   \n",
       "1    2.168  2.554  0.873  ...   1.0   1.0    1.0     1.0              1.0   \n",
       "2    1.642  2.099  0.824  ...   1.0   1.0    1.0     1.0              1.0   \n",
       "3    1.642  2.099  0.824  ...   1.0   1.0    1.0     1.0              1.0   \n",
       "4    1.584  2.003  0.813  ...   1.0   1.0    1.0     1.0              1.0   \n",
       "..     ...    ...    ...  ...   ...   ...    ...     ...              ...   \n",
       "801  2.425  2.855  0.859  ...   0.0   1.0    1.0     1.0              1.0   \n",
       "802  2.392  3.019  0.811  ...   1.0   1.0    1.0     1.0              1.0   \n",
       "803  2.392  3.019  0.811  ...   1.0   2.0    2.0     1.0              1.0   \n",
       "804  2.657  3.275  0.821  ...   1.0   1.0    1.0     1.0              1.0   \n",
       "805  2.657  3.275  0.821  ...   1.0   1.0    1.0     1.0              1.0   \n",
       "\n",
       "     DECAYED  WOOD  GRASS  FLOWER  CHEMICAL  \n",
       "0        1.0   1.0    1.0     1.0       2.0  \n",
       "1        1.0   1.0    1.0     1.0       2.0  \n",
       "2        1.0   1.0    1.0     1.0       2.0  \n",
       "3        1.0   1.0    1.0     1.0       1.0  \n",
       "4        1.0   1.0    1.0     1.0       1.0  \n",
       "..       ...   ...    ...     ...       ...  \n",
       "801      1.0   1.0    1.0     1.0       2.0  \n",
       "802      1.0   1.0    1.0     1.0       2.0  \n",
       "803      2.0   1.0    1.0     1.0       2.0  \n",
       "804      1.0   1.0    1.0     1.0       2.0  \n",
       "805      1.0   1.0    3.0     2.0       1.0  \n",
       "\n",
       "[2418 rows x 3104 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfX = pd.read_csv('Molecular Dataset Dream 2.csv',sep=';')\n",
    "dfY = pd.read_csv('Senteur Dataset Dream 2.csv',sep=';')\n",
    "df = dfX.merge(dfY)\n",
    "df1 = df\n",
    "df2 = df\n",
    "df3 = pd.concat([df1, df2])\n",
    "df= pd.concat([df, df3])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que la colonne 'Unnamed 0' qui sert à marger les 2 dataframe est complètement fausser. \n",
    "On se retrouve donc à prédire la classe 'INTENSITY/STRENGTH' avec les mauvais descripteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_labelY = ['INTENSITY/STRENGTH']\n",
    "list_label = ['INTENSITY/STRENGTH', 'VALENCE/PLEASANTNESS', 'BAKERY', 'SWEET',\n",
    "       'FRUIT', 'FISH', 'GARLIC', 'SPICES', 'COLD', 'SOUR', 'BURNT', 'ACID',\n",
    "       'WARM', 'MUSKY', 'SWEATY', 'AMMONIA/URINOUS', 'DECAYED', 'WOOD',\n",
    "       'GRASS', 'FLOWER', 'CHEMICAL']\n",
    "#df = df.sample(n=806)\n",
    "dfY = df[list_labelY]\n",
    "\n",
    "df = df.drop(list_label,axis=1)\n",
    "df = df.drop('Unnamed: 0',axis=1)\n",
    "df = df.drop('CID',axis=1)\n",
    "dfX = df\n",
    "len(list_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test sur le réseau neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complexity from pubmed</th>\n",
       "      <th>MW</th>\n",
       "      <th>AMW</th>\n",
       "      <th>Sv</th>\n",
       "      <th>Se</th>\n",
       "      <th>Sp</th>\n",
       "      <th>Si</th>\n",
       "      <th>Mv</th>\n",
       "      <th>Me</th>\n",
       "      <th>Mp</th>\n",
       "      <th>...</th>\n",
       "      <th>Hypertens-80</th>\n",
       "      <th>Hypertens-50</th>\n",
       "      <th>Hypnotic-80</th>\n",
       "      <th>Hypnotic-50</th>\n",
       "      <th>Neoplastic-80</th>\n",
       "      <th>Neoplastic-50</th>\n",
       "      <th>Infective-80</th>\n",
       "      <th>Infective-50</th>\n",
       "      <th>Compound Identifier</th>\n",
       "      <th>Intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.532</td>\n",
       "      <td>4.961</td>\n",
       "      <td>2.011</td>\n",
       "      <td>2.155</td>\n",
       "      <td>2.482</td>\n",
       "      <td>2.168</td>\n",
       "      <td>2.554</td>\n",
       "      <td>0.873</td>\n",
       "      <td>1.006</td>\n",
       "      <td>0.879</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.532</td>\n",
       "      <td>4.961</td>\n",
       "      <td>2.011</td>\n",
       "      <td>2.155</td>\n",
       "      <td>2.482</td>\n",
       "      <td>2.168</td>\n",
       "      <td>2.554</td>\n",
       "      <td>0.873</td>\n",
       "      <td>1.006</td>\n",
       "      <td>0.879</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.141</td>\n",
       "      <td>3.916</td>\n",
       "      <td>1.958</td>\n",
       "      <td>1.648</td>\n",
       "      <td>2.034</td>\n",
       "      <td>1.642</td>\n",
       "      <td>2.099</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1.017</td>\n",
       "      <td>0.821</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>176</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.141</td>\n",
       "      <td>3.916</td>\n",
       "      <td>1.958</td>\n",
       "      <td>1.648</td>\n",
       "      <td>2.034</td>\n",
       "      <td>1.642</td>\n",
       "      <td>2.099</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1.017</td>\n",
       "      <td>0.821</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>176</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.175</td>\n",
       "      <td>3.531</td>\n",
       "      <td>1.846</td>\n",
       "      <td>1.556</td>\n",
       "      <td>1.921</td>\n",
       "      <td>1.584</td>\n",
       "      <td>2.003</td>\n",
       "      <td>0.813</td>\n",
       "      <td>1.004</td>\n",
       "      <td>0.828</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>177</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>5.336</td>\n",
       "      <td>5.268</td>\n",
       "      <td>1.909</td>\n",
       "      <td>2.372</td>\n",
       "      <td>2.747</td>\n",
       "      <td>2.425</td>\n",
       "      <td>2.855</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.878</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6429333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>4.657</td>\n",
       "      <td>5.268</td>\n",
       "      <td>1.826</td>\n",
       "      <td>2.341</td>\n",
       "      <td>2.891</td>\n",
       "      <td>2.392</td>\n",
       "      <td>3.019</td>\n",
       "      <td>0.811</td>\n",
       "      <td>1.002</td>\n",
       "      <td>0.829</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6999977</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>4.657</td>\n",
       "      <td>5.268</td>\n",
       "      <td>1.826</td>\n",
       "      <td>2.341</td>\n",
       "      <td>2.891</td>\n",
       "      <td>2.392</td>\n",
       "      <td>3.019</td>\n",
       "      <td>0.811</td>\n",
       "      <td>1.002</td>\n",
       "      <td>0.829</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6999977</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>5.867</td>\n",
       "      <td>5.670</td>\n",
       "      <td>1.804</td>\n",
       "      <td>2.579</td>\n",
       "      <td>3.128</td>\n",
       "      <td>2.657</td>\n",
       "      <td>3.275</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.845</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16220109</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>5.867</td>\n",
       "      <td>5.670</td>\n",
       "      <td>1.804</td>\n",
       "      <td>2.579</td>\n",
       "      <td>3.128</td>\n",
       "      <td>2.657</td>\n",
       "      <td>3.275</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.845</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16220109</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2418 rows × 3081 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     complexity from pubmed     MW    AMW     Sv     Se     Sp     Si     Mv  \\\n",
       "0                     4.532  4.961  2.011  2.155  2.482  2.168  2.554  0.873   \n",
       "1                     4.532  4.961  2.011  2.155  2.482  2.168  2.554  0.873   \n",
       "2                     3.141  3.916  1.958  1.648  2.034  1.642  2.099  0.824   \n",
       "3                     3.141  3.916  1.958  1.648  2.034  1.642  2.099  0.824   \n",
       "4                     2.175  3.531  1.846  1.556  1.921  1.584  2.003  0.813   \n",
       "..                      ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "801                   5.336  5.268  1.909  2.372  2.747  2.425  2.855  0.859   \n",
       "802                   4.657  5.268  1.826  2.341  2.891  2.392  3.019  0.811   \n",
       "803                   4.657  5.268  1.826  2.341  2.891  2.392  3.019  0.811   \n",
       "804                   5.867  5.670  1.804  2.579  3.128  2.657  3.275  0.821   \n",
       "805                   5.867  5.670  1.804  2.579  3.128  2.657  3.275  0.821   \n",
       "\n",
       "        Me     Mp  ...  Hypertens-80  Hypertens-50  Hypnotic-80  Hypnotic-50  \\\n",
       "0    1.006  0.879  ...             0             0            0            0   \n",
       "1    1.006  0.879  ...             0             0            0            0   \n",
       "2    1.017  0.821  ...             0             0            0            0   \n",
       "3    1.017  0.821  ...             0             0            0            0   \n",
       "4    1.004  0.828  ...             0             0            0            0   \n",
       "..     ...    ...  ...           ...           ...          ...          ...   \n",
       "801  0.995  0.878  ...             0             0            0            0   \n",
       "802  1.002  0.829  ...             0             0            0            0   \n",
       "803  1.002  0.829  ...             0             0            0            0   \n",
       "804  0.995  0.845  ...             0             0            1            0   \n",
       "805  0.995  0.845  ...             0             0            1            0   \n",
       "\n",
       "     Neoplastic-80  Neoplastic-50  Infective-80  Infective-50  \\\n",
       "0                0              0             0             0   \n",
       "1                0              0             0             0   \n",
       "2                0              0             0             0   \n",
       "3                0              0             0             0   \n",
       "4                0              0             0             0   \n",
       "..             ...            ...           ...           ...   \n",
       "801              0              0             1             0   \n",
       "802              0              0             0             0   \n",
       "803              0              0             0             0   \n",
       "804              1              0             1             0   \n",
       "805              1              0             1             0   \n",
       "\n",
       "     Compound Identifier  Intensity  \n",
       "0                    126          0  \n",
       "1                    126          1  \n",
       "2                    176          0  \n",
       "3                    176          1  \n",
       "4                    177          0  \n",
       "..                   ...        ...  \n",
       "801              6429333          0  \n",
       "802              6999977          1  \n",
       "803              6999977          0  \n",
       "804             16220109          0  \n",
       "805             16220109          1  \n",
       "\n",
       "[2418 rows x 3081 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_class(y_):\n",
    "    res = np.zeros((len(y_), 11), dtype='q')\n",
    "    for i in range(len(y_)):\n",
    "        res[i][int(y_[i])] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2418, 11)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataX = np.float32(dfX.values)\n",
    "dataY = np.float32(dfY.values)\n",
    "#dataY = np.longlong(dfY.values)#pour nn.NLLLoss()\n",
    "L = []\n",
    "for i in range(806):\n",
    "    L.append(dataY[i][0])\n",
    "len(set(L))\n",
    "dataY = to_class(dataY)\n",
    "dataY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation\n",
    "sc = StandardScaler()\n",
    "dataX = sc.fit_transform(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taille du data test\n",
    "SPLIT = 0.1\n",
    "Nb_splt= int(len(dataX)-len(dataX)*SPLIT)\n",
    "\n",
    "DATA_Train= dataX [0:Nb_splt]\n",
    "DATA_Test= dataX [Nb_splt:len(dataX)]\n",
    "\n",
    "TARGET_Train= dataY[0:Nb_splt]\n",
    "TARGET_Test= dataY[Nb_splt:len(dataX)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PyTorch is used to working with batches.\n",
    "Batch=60\n",
    "DATA_Train, DATA_Test, TARGET_Train, TARGET_Test = train_test_split(dataX, dataY ,test_size=0.1)\n",
    "\n",
    "X_train_tensor = torch.from_numpy(DATA_Train)\n",
    "Y_train_tensor = torch.from_numpy(TARGET_Train) \n",
    "\n",
    "X_test_tensor = torch.from_numpy(DATA_Test)\n",
    "Y_test_tensor = torch.from_numpy(TARGET_Test)\n",
    "\n",
    "train = data_utils.TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "train_loader = data_utils.DataLoader(train, batch_size=Batch, shuffle=True)\n",
    "\n",
    "test = data_utils.TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "test_loader = data_utils.DataLoader(test, batch_size=Batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        \n",
    "        ## Activation layer\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = 3081, out_features = 2400)\n",
    "        self.fc2 = nn.Linear(2400, 1200)  \n",
    "        self.fc3 = nn.Linear(1200, 600)\n",
    "        \n",
    "        self.output = nn.Linear(600, 11)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "    ## First full connection\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "    \n",
    "    ## Second full connection\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    " \n",
    "    ## Third full connection\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "    ## Output layer\n",
    "        x = self.output(x)\n",
    "        y = self.softmax(x)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2176, 3081])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an instance of our network\n",
    "net = MyNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MyNetwork()\n",
    "net = net.cuda()\n",
    "\n",
    "LEARNING_RATE = 0.003\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "#criterion = nn.CrossEntropyLoss\n",
    "\n",
    "# Méthode stochastique de descente du grandient \n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n",
      "Training accuracy: 0.17003676470588236 Training loss: 0.039072884904111135\n",
      "Validation accuracy: 0.16942148760330578 Validation loss: 0.04538140513680198\n",
      "Temps de l Epoch  0  en secondes: 0.5784246921539307\n",
      "EPOCH: 1\n",
      "Training accuracy: 0.20404411764705882 Training loss: 0.03578019679030951\n",
      "Validation accuracy: 0.25206611570247933 Validation loss: 0.041105668406841184\n",
      "Temps de l Epoch  1  en secondes: 0.33867931365966797\n",
      "EPOCH: 2\n",
      "Training accuracy: 0.22472426470588236 Training loss: 0.034466820056824124\n",
      "Validation accuracy: 0.2768595041322314 Validation loss: 0.04150421116962906\n",
      "Temps de l Epoch  2  en secondes: 0.33011555671691895\n",
      "EPOCH: 3\n",
      "Training accuracy: 0.25965073529411764 Training loss: 0.03372239787131548\n",
      "Validation accuracy: 0.256198347107438 Validation loss: 0.04067124414049889\n",
      "Temps de l Epoch  3  en secondes: 0.3261275291442871\n",
      "EPOCH: 4\n",
      "Training accuracy: 0.25827205882352944 Training loss: 0.033111212665543836\n",
      "Validation accuracy: 0.2892561983471074 Validation loss: 0.04022403738715432\n",
      "Temps de l Epoch  4  en secondes: 0.32839012145996094\n",
      "EPOCH: 5\n",
      "Training accuracy: 0.2853860294117647 Training loss: 0.032336972653865814\n",
      "Validation accuracy: 0.29338842975206614 Validation loss: 0.04050716782404371\n",
      "Temps de l Epoch  5  en secondes: 0.3238534927368164\n",
      "EPOCH: 6\n",
      "Training accuracy: 0.3125 Training loss: 0.031393883749842644\n",
      "Validation accuracy: 0.2727272727272727 Validation loss: 0.03837560425119952\n",
      "Temps de l Epoch  6  en secondes: 0.32204270362854004\n",
      "EPOCH: 7\n",
      "Training accuracy: 0.3249080882352941 Training loss: 0.030458168838830554\n",
      "Validation accuracy: 0.30165289256198347 Validation loss: 0.03672469007082222\n",
      "Temps de l Epoch  7  en secondes: 0.3272819519042969\n",
      "EPOCH: 8\n",
      "Training accuracy: 0.3713235294117647 Training loss: 0.029138107784092426\n",
      "Validation accuracy: 0.2727272727272727 Validation loss: 0.03840793312088517\n",
      "Temps de l Epoch  8  en secondes: 0.3355743885040283\n",
      "EPOCH: 9\n",
      "Training accuracy: 0.4016544117647059 Training loss: 0.027582578361034393\n",
      "Validation accuracy: 0.3181818181818182 Validation loss: 0.036485258705359846\n",
      "Temps de l Epoch  9  en secondes: 0.3269619941711426\n",
      "EPOCH: 10\n",
      "Training accuracy: 0.4352022058823529 Training loss: 0.02619390463566079\n",
      "Validation accuracy: 0.3140495867768595 Validation loss: 0.03760670433359698\n",
      "Temps de l Epoch  10  en secondes: 0.3211252689361572\n",
      "EPOCH: 11\n",
      "Training accuracy: 0.4600183823529412 Training loss: 0.024893838638330206\n",
      "Validation accuracy: 0.359504132231405 Validation loss: 0.03706983249049541\n",
      "Temps de l Epoch  11  en secondes: 0.3236198425292969\n",
      "EPOCH: 12\n",
      "Training accuracy: 0.5220588235294118 Training loss: 0.02258290142259177\n",
      "Validation accuracy: 0.40082644628099173 Validation loss: 0.02779890855481802\n",
      "Temps de l Epoch  12  en secondes: 0.32413244247436523\n",
      "EPOCH: 13\n",
      "Training accuracy: 0.5845588235294118 Training loss: 0.020285782897297072\n",
      "Validation accuracy: 0.4297520661157025 Validation loss: 0.03609476158441591\n",
      "Temps de l Epoch  13  en secondes: 0.3269200325012207\n",
      "EPOCH: 14\n",
      "Training accuracy: 0.6148897058823529 Training loss: 0.018491974346997106\n",
      "Validation accuracy: 0.5247933884297521 Validation loss: 0.027160001195166723\n",
      "Temps de l Epoch  14  en secondes: 0.3266165256500244\n",
      "EPOCH: 15\n",
      "Training accuracy: 0.6677389705882353 Training loss: 0.016497291843680775\n",
      "Validation accuracy: 0.5826446280991735 Validation loss: 0.02452526851133867\n",
      "Temps de l Epoch  15  en secondes: 0.3204386234283447\n",
      "EPOCH: 16\n",
      "Training accuracy: 0.7100183823529411 Training loss: 0.014917725368457683\n",
      "Validation accuracy: 0.6115702479338843 Validation loss: 0.018783043977642848\n",
      "Temps de l Epoch  16  en secondes: 0.3245704174041748\n",
      "EPOCH: 17\n",
      "Training accuracy: 0.7927389705882353 Training loss: 0.012054929674109992\n",
      "Validation accuracy: 0.6942148760330579 Validation loss: 0.018299876165784094\n",
      "Temps de l Epoch  17  en secondes: 0.326993465423584\n",
      "EPOCH: 18\n",
      "Training accuracy: 0.8033088235294118 Training loss: 0.011202427106635535\n",
      "Validation accuracy: 0.7024793388429752 Validation loss: 0.014487243142009767\n",
      "Temps de l Epoch  18  en secondes: 0.3294675350189209\n",
      "EPOCH: 19\n",
      "Training accuracy: 0.7766544117647058 Training loss: 0.011436770738595548\n",
      "Validation accuracy: 0.7479338842975206 Validation loss: 0.01764560016718778\n",
      "Temps de l Epoch  19  en secondes: 0.3365805149078369\n",
      "EPOCH: 20\n",
      "Training accuracy: 0.8616727941176471 Training loss: 0.008298170136506943\n",
      "Validation accuracy: 0.7727272727272727 Validation loss: 0.012059612953958432\n",
      "Temps de l Epoch  20  en secondes: 0.32316040992736816\n",
      "EPOCH: 21\n",
      "Training accuracy: 0.8809742647058824 Training loss: 0.007112120880800135\n",
      "Validation accuracy: 0.8223140495867769 Validation loss: 0.009963452569709336\n",
      "Temps de l Epoch  21  en secondes: 0.32993221282958984\n",
      "EPOCH: 22\n",
      "Training accuracy: 0.9021139705882353 Training loss: 0.006510239861467306\n",
      "Validation accuracy: 0.8471074380165289 Validation loss: 0.00864268137403756\n",
      "Temps de l Epoch  22  en secondes: 0.32385826110839844\n",
      "EPOCH: 23\n",
      "Training accuracy: 0.9168198529411765 Training loss: 0.005461367819567814\n",
      "Validation accuracy: 0.8181818181818182 Validation loss: 0.01689280951318662\n",
      "Temps de l Epoch  23  en secondes: 0.3271055221557617\n",
      "EPOCH: 24\n",
      "Training accuracy: 0.9025735294117647 Training loss: 0.006166970275123329\n",
      "Validation accuracy: 0.8429752066115702 Validation loss: 0.009976143309892702\n",
      "Temps de l Epoch  24  en secondes: 0.3340871334075928\n",
      "EPOCH: 25\n",
      "Training accuracy: 0.8713235294117647 Training loss: 0.007644628804615315\n",
      "Validation accuracy: 0.7727272727272727 Validation loss: 0.011212389331218625\n",
      "Temps de l Epoch  25  en secondes: 0.3361365795135498\n",
      "EPOCH: 26\n",
      "Training accuracy: 0.8841911764705882 Training loss: 0.006454432588618468\n",
      "Validation accuracy: 0.8677685950413223 Validation loss: 0.008353292572596841\n",
      "Temps de l Epoch  26  en secondes: 0.3221008777618408\n",
      "EPOCH: 27\n",
      "Training accuracy: 0.9044117647058824 Training loss: 0.0054561855778207675\n",
      "Validation accuracy: 0.8884297520661157 Validation loss: 0.008356380992191883\n",
      "Temps de l Epoch  27  en secondes: 0.3248627185821533\n",
      "EPOCH: 28\n",
      "Training accuracy: 0.9319852941176471 Training loss: 0.003918114114854047\n",
      "Validation accuracy: 0.9008264462809917 Validation loss: 0.005672858335262488\n",
      "Temps de l Epoch  28  en secondes: 0.3263065814971924\n",
      "EPOCH: 29\n",
      "Training accuracy: 0.8998161764705882 Training loss: 0.005394716795040842\n",
      "Validation accuracy: 0.8801652892561983 Validation loss: 0.008427666607967094\n",
      "Temps de l Epoch  29  en secondes: 0.3301503658294678\n",
      "Temps total en secondes: 10.087480306625366\n"
     ]
    }
   ],
   "source": [
    "## Nombre d'époque d'apprentissage\n",
    "N_EPOCHS = 30\n",
    "\n",
    "epoch_loss, epoch_acc, epoch_val_loss, epoch_val_acc = [], [], [], []\n",
    "start_time = time.time()\n",
    "\n",
    "for e in range(N_EPOCHS):\n",
    "    print(\"EPOCH:\",e)\n",
    "    ### boucle d'entraineement\n",
    "    running_loss = 0\n",
    "    running_accuracy = 0\n",
    "    running_acc=0\n",
    "    start_epoch_time=time.time()\n",
    "\n",
    "    ## Le réseau est mis en mode \"entrainement\"\n",
    "    net.train()\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "    \n",
    "        # Obtenir batch du dataloader\n",
    "        x = batch[0]\n",
    "        labels = batch[1]\n",
    "        # déplacer le batch sur le GPU\n",
    "        x = x.cuda()\n",
    "        labels = labels.cuda()\n",
    "        # Calcul de l'output et les loss\n",
    "        output = net(x)\n",
    "        y = output\n",
    "        loss = criterion(y, torch.max(labels, 1)[1])\n",
    "        #loss = criterion(y, labels)\n",
    "\n",
    "        # Réinitialisation du gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Calculs du gradients\n",
    "        loss.backward()\n",
    "        # Appliquecation d'une étape d'optimisation de l'algorithme de descente pour mettre à jour les poids\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            running_loss += loss.item()            \n",
    "            running_accuracy += (y.max(1)[1] == torch.max(labels, 1)[1]).sum().item()\n",
    "    print(\"Training accuracy:\", running_accuracy/float(len(train)),\n",
    "        \"Training loss:\", running_loss/float(len(train)))\n",
    "    epoch_loss.append(running_loss/len(train))\n",
    "    epoch_acc.append(running_accuracy/len(train))\n",
    "    ### Boucle de valisation\n",
    "    ## Le réseau est mis en mode validation\n",
    "    net.eval()\n",
    "    running_val_loss = 0\n",
    "    running_val_accuracy = 0\n",
    "\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            x = batch[0]\n",
    "            labels = batch[1]\n",
    "            x = x.cuda()\n",
    "            labels = labels.cuda()\n",
    "            output = net(x)\n",
    "            y = output\n",
    "            loss = criterion(y, torch.max(labels, 1)[1])\n",
    "            running_val_loss += loss.item()\n",
    "            running_val_accuracy += (y.max(1)[1] == torch.max(labels, 1)[1]).sum().item()\n",
    "    \n",
    "    print(\"Validation accuracy:\", running_val_accuracy/float(len(test)),\n",
    "        \"Validation loss:\", running_val_loss/float(len(test)))\n",
    "\n",
    "    epoch_val_loss.append(running_val_loss/len(test))\n",
    "    epoch_val_acc.append(running_val_accuracy/len(test))\n",
    "    inter = time.time() - start_epoch_time  \n",
    "    print ('Temps de l Epoch ',e,' en secondes:', inter )\n",
    "interval = time.time() - start_time  \n",
    "print ('Temps total en secondes:', interval )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(0,N_EPOCHS),epoch_acc)\n",
    "plt.plot(np.arange(0,N_EPOCHS),epoch_val_acc)\n",
    "plt.title(\"Evolution de l'accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Acc\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0,N_EPOCHS),epoch_loss)\n",
    "plt.plot(np.arange(0,N_EPOCHS),epoch_val_loss)\n",
    "plt.title(\"Evolution du loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application au data test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd_Test = pd.read_csv('datatest.csv',sep=';')\n",
    "pd_Test = pd_Test.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd_Test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Test= np.float32(pd_Test.values)\n",
    "X_test_tensor = torch.from_numpy(Test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_test_tensor.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "net = net.cuda()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X = X_test_tensor.cuda()\n",
    "LF=net(X)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "LF.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "LF= LF.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "LF=np.argmax(LF, axis = 1)\n",
    "Predict= pd.DataFrame(LF)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Label_moyen= pd.DataFrame(columns = ['CID','Label'])\n",
    "for i in range (68):\n",
    "    S =0\n",
    "    for j in range (49):\n",
    "        cpt = i *49 + j\n",
    "        S += Predict.iat[cpt,0]\n",
    "    S = S /49\n",
    "    Label_moyen.loc[i] = {'CID': pd_Test.iat[cpt,0], 'Label': S}\n",
    "    \n",
    "for i in range(68):\n",
    "    Label_moyen.iat[i,1] = (Label_moyen.iat[i,1] // 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Label_moyen.to_csv('predrictionNN_dilution.csv',sep=';')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Gerkins = [180 , 28.616,262 , 29.028,264 , 53.979,323 , 61.859,702 , 16.045,750 , 15.423,753 , 16.074,\n",
    "           1031 , 23.755,3102 , 48.555,3314 , 43.897,5862 , 60.523,5962 , 28.766,6274 , 25.361,6322 , 23.289,\n",
    "           6506 , 41.89,6544 , 58.077,6561 , 34.986,6669 , 40.716,7092 , 56.12,7137 , 41.858,7302 , 28.195,\n",
    "           7476 , 53.551,7559 , 62.397, 7657 , 33.909,7770 , 64.112,7793 , 63.547,7797 , 52.343,8025 , 21.99,\n",
    "           8049 , 22.308,8094 , 62.55,8419 , 56.935,8438 , 61.708,8468 , 41.512,8815 , 57.883,8878 , 51.215,\n",
    "           9012 , 68.552,10886 , 33.466,11567 , 65.368,12020 , 45.874,12265 , 54.837,12377 , 75.957,15654 , 69.017,\n",
    "           16537 , 44.404,17898 , 66.215,18467 , 61.782,21363 , 51.037,27440 , 70.045,31219 , 53.544,31276 ,\n",
    "           58.361,31283 , 13.462,60998 , 42.546,61523 , 51.003,62089 , 49.671,62351 , 76.057,62465 , 58.715,\n",
    "           251531 , 28.475,440917 , 52.775,637758 , 48.085,853433 , 42.768,1549025 , 31.097,2733294 , 57.642,\n",
    "           5281167 , 54.421,5281168 , 55.771,5318599 , 62.599,5352837 , 65.443,5364231 , 52.482,5371102 , 27.657,\n",
    "           10857465 , 32.66]\n",
    "Label_Gerkins= pd.DataFrame(columns = ['CID','Label'])\n",
    "for i in range(68):\n",
    "    Label_Gerkins.loc[i] = {'CID': Gerkins[2*i], 'Label': Gerkins[2*i+1]}\n",
    "\n",
    "for i in range(68):\n",
    "    if Label_Gerkins.iat[i,1] == 0:\n",
    "        Label_Gerkins.iat[i,1] = 0\n",
    "    else :\n",
    "        Label_Gerkins.iat[i,1] = (Label_Gerkins.iat[i,1] // 10) +1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Label_moyen"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Label_Gerkins"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Acc =0\n",
    "for i in range(68):\n",
    "    if Label_Gerkins.iat[i,1] == Label_moyen.iat[i,1] :\n",
    "        Acc+=1\n",
    "ACc = Acc/68\n",
    "print('Notre accuracy par rapport à Gerkins sur le Data_Test_2 est de:',ACc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adamax, Nadam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import exp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dfX #features\n",
    "y_train = dfY #labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_array(X_AT_train, y_AT_train):\n",
    "    training_in = np.empty([len(X_AT_train), 4872], dtype = float) #features inputs\n",
    "    training_out = np.empty([len(y_AT_train), 1], dtype = float) #labels outputs\n",
    "    for i in range(len(X_AT_train)): #1280\n",
    "        training_out[i] = y_AT_train[i]\n",
    "        for j in range(len(list_label)): #10\n",
    "            training_in[i][j] = X_AT_train[list_label[j]][i]\n",
    "    return(training_in, training_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.float32(dfX.values)\n",
    "Y_train = np.float32(dfY.values)\n",
    "\n",
    "x_train,x_test,y_train_,y_test_ = train_test_split(X_train,Y_train,test_size=0.2)\n",
    "y_train = to_class(y_train_)\n",
    "y_test = to_class(y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(2400, activation='relu', input_dim=3120))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1200, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(600, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(12, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=30, batch_size=300, validation_data=(x_test,y_test), shuffle=True)\n",
    "model.test_on_batch(x_test, y_test)\n",
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  \"Accuracy\"\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
